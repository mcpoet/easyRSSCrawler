1，要写一个爬虫，把新闻都扒下来，然后存到服务器里。。。。
2，编辑器会处理掉这些新闻，存到新的数据库中，新的数据库再把处理过的数据推送到客户端。。。
3，OK，爬虫，爬新闻是好的，存新闻也是好的，
4，编辑器取新闻就是顺手拈来的了，
5，只是，python 爬虫要搭单独的server么？
6，单独搭是优势呢，由编辑器做一个中转也好，内网用更合适。
7，爬虫呢，爬有目标，也有逻辑，也有存储，存储用MongoDB倒也是靠谱的。
8，爬虫的顺序，既是从数据库，到基本功能，到自动爬取的实现。
9，也就是这三步走，1），数据存储方案。2），基本框架，tornado Or Something， 基本功能的实现；3），自动爬取逻辑；4）编辑器接口。
10，数据库的选择：Couch，GoD，MongoDB，Redis，MySQL。
11，数据的特征：Big Table， One Dimension，同源同性；合适的选择：MongoDB，MySQL，磁盘为主，独立主机；
12，服务器的部署，可以分布式，可增量，可离线，可推送；
13，	这点上而言，Cassandra，MongoDB和MySQL都是不错的选择，为了保持同源性？MongoDB是不错的选择，
	备选的方案是Redis，或者，直接写到文件里都可以。。。
14，	为了保持同源性，先用MongoDB。
15，好了，第一步，把爬下来的数据，存到数据库里面，每五分钟或者10分钟，更新一次。

15.5，Maybe 我们可以考虑一下rawDog用来抓数据神马的。

16，	爬虫写好了，但是编辑器的API和回写还是要的：
	编辑器的API，下载RSS信息，上传被编辑过的RSS条目；
	同时，被编辑的条目会转化成主题，被传入另外一个数据库，之前的那个数据库。

	基本上就是三个API了，下载GET，上传POST，这两个都用Python写么？需要一个WebServer？
	第三个直接转到127.0.0.1的scalaBIRD上面就好了；
	好主意是，把两个API转到scalaBird上面；只是这样的架构有点混搭，sort of wierd.

	就这么定了，所有JSON-HTTP API 都用scalaBIRD来做；python就做crawler那点事。